{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "#tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10\n",
    "numb_of_neurons=512\n",
    "learning_rate=0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])# MNIST data image of shape 28 * 28 = 784\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])# 0 - 9 digits recognition = 10 classes\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, numb_of_neurons],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([numb_of_neurons]))\n",
    "W2 = tf.get_variable(\"W2\", shape=[numb_of_neurons, numb_of_neurons],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([numb_of_neurons]))\n",
    "W3 = tf.get_variable(\"W3\", shape=[numb_of_neurons, numb_of_neurons],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([numb_of_neurons]))\n",
    "W4 = tf.get_variable(\"W4\", shape=[numb_of_neurons, numb_of_neurons],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([numb_of_neurons]))\n",
    "W5 = tf.get_variable(\"W5\", shape=[numb_of_neurons, nb_classes],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "with tf.name_scope(\"layer1\") as scope:\n",
    "    # Hypothesis (using Relu)\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "    \n",
    "with tf.name_scope(\"layer2\") as scope:\n",
    "    # Hypothesis (using Relu)\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "with tf.name_scope(\"layer3\") as scope:\n",
    "    # Hypothesis (using Relu)\n",
    "    L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "    L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "    \n",
    "with tf.name_scope(\"layer4\") as scope:\n",
    "    # Hypothesis (using Relu)\n",
    "    L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "    L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "with tf.name_scope(\"layer5\") as scope:\n",
    "    # Hypothesis (using softmax)\n",
    "    #hypothesis = tf.nn.softmax(tf.matmul(layer2, W3) + b3)\n",
    "    hypothesis = tf.matmul(L4, W5) + b5\n",
    "    \n",
    "\n",
    "with tf.name_scope(\"cost\") as scope:\n",
    "    #cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, l   abels=Y))\n",
    "    cost_summ=tf.summary.scalar(\"cost\",cost)\n",
    "    \n",
    "with tf.name_scope(\"train\") as scope:    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(cost)\n",
    "    #GradientDescent doesn't work well.. I don't why Adam is better than Gradient.\n",
    "\n",
    "is_correct= tf.equal(tf.arg_max(hypothesis,1),tf.argmax(Y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "training_epochs=15\n",
    "batch_size=500\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    merged_summary= tf.summary.merge_all()\n",
    "    #step3 create wirter after creting session\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer=tf.summary.FileWriter('/home/jeongjaepark/HistNet/logs')\n",
    "    writer.add_graph(sess.graph)\n",
    "    step=0\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost=0\n",
    "        total_batch=int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            feed_dict={X: batch_xs, Y:batch_ys,keep_prob:0.7}\n",
    "            c, _ = sess.run([cost, optimizer],feed_dict=feed_dict)\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "            s=sess.run(merged_summary, feed_dict=feed_dict)\n",
    "            writer.add_summary(s,global_step=step)\n",
    "            step+=1\n",
    "        \n",
    "        print(\"Epoch = \", '%4d'%(epoch+1),'cost = ','{:.9f}'.format(avg_cost))\n",
    "    # Test the model using test sets\n",
    "    \n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels, keep_prob:1}))        \n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "    print(\"Prediction: \", sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1],keep_prob:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lab 7 Learning rate and Evaluation\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lab 10 MNIST and NN\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from skimage import data, io, filters\n",
    "from mlxtend.preprocessing import one_hot\n",
    "import random\n",
    "\n",
    "# set train data path\n",
    "train_list_path = 'data/images_path/train.txt'\n",
    "train_data_path = 'data/images/train/%s'\n",
    "\n",
    "\n",
    "class DataRead:\n",
    "    def __init__(self, data_list_path, data_path, batch_size, nb_classes):\n",
    "        self.data_list_path = data_list_path\n",
    "        self.data_path = data_path\n",
    "        self.batch_num = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_classes = nb_classes\n",
    "\n",
    "        with open(self.data_list_path) as f:\n",
    "            self.data_list = f.readlines()\n",
    "        self.total_batch = int(np.floor(np.size(self.data_list) / self.batch_size))\n",
    "\n",
    "        self.label_list = [int(d.split()[1]) for d in self.data_list]\n",
    "        self.data_list = [train_data_path % d.split()[0] for d in self.data_list]\n",
    "\n",
    "        # shuffle data\n",
    "        ind_shuff = np.arange(np.size(self.data_list))\n",
    "        random.shuffle(ind_shuff)\n",
    "        self.data_list = [self.data_list[i] for i in ind_shuff]\n",
    "        self.label_list = [self.label_list[i] for i in ind_shuff]\n",
    "\n",
    "    def Next_Batch(self):\n",
    "        batch_num = int(self.batch_num)\n",
    "        batch_size = int(self.batch_size)\n",
    "\n",
    "        data_coll = io.ImageCollection(\n",
    "            self.data_list[batch_num * batch_size:(batch_num + 1) * batch_size])\n",
    "        data = np.array([d for d in data_coll])\n",
    "\n",
    "        label_coll = self.label_list[batch_num * batch_size:(batch_num + 1) * batch_size]\n",
    "        label = one_hot(label_coll, self.nb_classes)\n",
    "\n",
    "        self.batch_num = self.batch_num + 1\n",
    "        if self.batch_num > self.total_batch:\n",
    "            self.batch_num = 0\n",
    "        return (data / 255.0, label)\n",
    "\n",
    "    def All_Data(self):\n",
    "        data_coll = io.ImageCollection(\n",
    "            self.data_list)\n",
    "        data = np.array([d for d in data_coll])\n",
    "\n",
    "        label_coll = self.label_list\n",
    "        label = one_hot(label_coll, self.nb_classes)\n",
    "\n",
    "        return (data / 255.0, label)\n",
    "\n",
    "\n",
    "nb_classes = 13\n",
    "learning_rate = 0.001\n",
    "training_epochs = 45\n",
    "batch_size = 256\n",
    "train = DataRead(train_list_path, train_data_path, batch_size=batch_size, nb_classes=13)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 256, 256, 3])\n",
    "X_img = X\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([11, 11, 3, 96], stddev=0.001))\n",
    "W2 = tf.Variable(tf.random_normal([5, 5, 96, 256], stddev=0.001))\n",
    "W3 = tf.Variable(tf.random_normal([3, 3, 256, 384], stddev=0.001))\n",
    "W4 = tf.Variable(tf.random_normal([3, 3, 384, 384], stddev=0.001))\n",
    "W5 = tf.Variable(tf.random_normal([3, 3, 384, 256], stddev=0.001))\n",
    "W6 = tf.get_variable(\"W6\", shape=[8 * 8 * 256, 4096], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([4096]))\n",
    "W7 = tf.get_variable(\"W7\", shape=[4096, 1000], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([1000]))\n",
    "W8 = tf.get_variable(\"W8\", shape=[1000, 100], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([100]))\n",
    "W9 = tf.get_variable(\"W9\", shape=[100, nb_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b9 = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "with tf.name_scope(\"layer1\") as scope:\n",
    "    L1 = tf.nn.conv2d(X_img, W1, strides=[1, 4, 4, 1], padding='SAME')\n",
    "    L1 = tf.nn.relu(L1)\n",
    "    L1 = tf.nn.max_pool(L1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "with tf.name_scope(\"layer2\") as scope:\n",
    "    L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    L2 = tf.nn.relu(L2)\n",
    "    L2 = tf.nn.max_pool(L2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "with tf.name_scope(\"layer3\") as scope:\n",
    "    L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    L3 = tf.nn.relu(L3)\n",
    "    L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "with tf.name_scope(\"layer4\") as scope:\n",
    "    L4 = tf.nn.conv2d(L3, W4, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    L4 = tf.nn.relu(L4)\n",
    "    L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "with tf.name_scope(\"layer5\") as scope:\n",
    "    L5 = tf.nn.conv2d(L4, W5, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    L5 = tf.nn.relu(L5)\n",
    "    L5 = tf.nn.max_pool(L5, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    L5 = tf.reshape(L5, [-1, 8 * 8 * 256])\n",
    "    L5 = tf.nn.dropout(L5, keep_prob=keep_prob)\n",
    "\n",
    "with tf.name_scope(\"layer6\") as scope:\n",
    "    L6 = tf.nn.relu(tf.matmul(L5, W6) + b6)\n",
    "    L6 = tf.nn.dropout(L6, keep_prob=keep_prob)\n",
    "\n",
    "with tf.name_scope(\"layer7\") as scope:\n",
    "    L7 = tf.nn.relu(tf.matmul(L6, W7) + b7)\n",
    "    L7 = tf.nn.dropout(L7, keep_prob=keep_prob)\n",
    "\n",
    "with tf.name_scope(\"layer8\") as scope:\n",
    "    L8 = tf.nn.relu(tf.matmul(L7, W8) + b8)\n",
    "    L8 = tf.nn.dropout(L8, keep_prob=keep_prob)\n",
    "\n",
    "with tf.name_scope(\"layer9\") as scope:\n",
    "    logits = tf.matmul(L8, W9) + b9\n",
    "\n",
    "with tf.name_scope(\"cost\") as scope:\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "    cost_summ=tf.summary.scalar(\"cost\",cost)\n",
    "    \n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.name_scope(\"prediction\") as scope:\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    accuracy_summ=tf.summary.scalar(\"accuracy\",accuracy)\n",
    "\n",
    "\n",
    "# train my model\n",
    "with tf.Session() as sess:\n",
    "    print('Learning started. It takes sometime.')\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter('./logs/KIST/learning_rate_0_001')\n",
    "    writer.add_graph(sess.graph)\n",
    "    step = 0\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        avg_a=0\n",
    "        total_batch = train.total_batch\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = train.Next_Batch()\n",
    "            feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "            c, _ ,a= sess.run([cost, optimizer,accuracy], feed_dict=feed_dict)\n",
    "            avg_cost += c / total_batch\n",
    "            avg_a+=a / total_batch\n",
    "            \n",
    "            s=sess.run(merged_summary, feed_dict=feed_dict)\n",
    "            writer.add_summary(s, global_step=step)\n",
    "            step += 1\n",
    "\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost),'accuracy =', '{:.9f}'.format(avg_a))\n",
    "\n",
    "    val_list_path = 'data/images_path/val.txt'\n",
    "    val_data_path = 'data/images/val/%s'\n",
    "    val = DataRead(val_list_path, val_data_path, batch_size=100, nb_classes=13)\n",
    "\n",
    "    val_x, val_y = val.All_Data()\n",
    "    print('Accuracy:', sess.run(accuracy, feed_dict={X: val_x, Y: val_y, keep_prob: 1}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from skimage import data, io, filters\n",
    "from mlxtend.preprocessing import one_hot\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import glob,os\n",
    "\n",
    "\n",
    "dataset_path=\"/home/jeongjaepark/HistNet/data/Maps1\"\n",
    "\n",
    "#read data_list\n",
    "os.chdir(dataset_path)\n",
    "files=[file for file in glob.glob(\"*.txt\")]\n",
    "NumofTest=int(len(files)*0.2)\n",
    "data_path=[dataset_path+'/'+f for f in files]\n",
    "\n",
    "class DataRead:\n",
    "    def __init__(self, data_path,batch_size,nb_classes,k_fold=10):\n",
    "        self.data_path=data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_classes=nb_classes\n",
    "        first=1\n",
    "        \n",
    "        #data read & preprocessing\n",
    "        for j,file_list in enumerate(data_path):\n",
    "            with open(file_list) as f:\n",
    "                tmp=f.readlines()\n",
    "                f.close()\n",
    "\n",
    "            #data preprocessing\n",
    "            data_string=[d.split(\",\")[:] for d in tmp]\n",
    "            data_len=np.size(data_string,0)\n",
    "            for i in range(data_len):\n",
    "                data_string[i][-1]=float(data_string[i][-1]=='t\\n')\n",
    "            tmp=np.array([[float(d) for d in line ]for line in data_string])\n",
    "            \n",
    "            #shuffle\n",
    "            ind_shuff=np.arange(data_len)\n",
    "            random.shuffle(ind_shuff)\n",
    "            \n",
    "            if(first):\n",
    "                data=tmp[ind_shuff,:]\n",
    "                count=[data_len]\n",
    "                data_ind=[0]\n",
    "                first=0\n",
    "            else:\n",
    "                data=np.append(data,tmp[ind_shuff,:],axis=0)\n",
    "                count=np.append(count,data_len)\n",
    "                data_ind=np.append(data_ind,data_ind[j-1]+count[j-1])             \n",
    "                \n",
    "        \n",
    "        self.data=data\n",
    "        self.count=count\n",
    "        self.data_ind=data_ind\n",
    "        \n",
    "    def data_select(self,data_number):\n",
    "        ind=np.ones(len(self.count), dtype=bool)\n",
    "        ind[data_number]=False\n",
    "        train_ind_s=self.data_ind[ind]\n",
    "        train_ind_n=self.count[ind]\n",
    "        \n",
    "        test_ind_s=self.data_ind[data_number]\n",
    "        test_ind_n=self.count[data_number]\n",
    "        \n",
    "        self.train_ind=[]\n",
    "        for i,start in enumerate(train_ind_s):\n",
    "            self.train_ind.extend([j for j in range(start,start+train_ind_n[i])])\n",
    "            \n",
    "        self.test_ind=[]\n",
    "        for i,start in enumerate(test_ind_s):\n",
    "            self.test_ind.extend([j for j in range(start,start+test_ind_n[i])])\n",
    "        \n",
    "        self.train=DataRead.distribution(self.data[self.train_ind,:].copy(),self.batch_size,self.nb_classes)\n",
    "        self.test=DataRead.distribution(self.data[self.test_ind,:].copy(),self.batch_size,self.nb_classes)\n",
    "    \n",
    "    class distribution:\n",
    "        def __init__(self,data,batch_size,nb_classes):\n",
    "            self.batch_size=int(batch_size)\n",
    "            self.batch_num=0\n",
    "            self.data_len=np.size(data,0)\n",
    "            \n",
    "            ind_shuff=np.arange(self.data_len)\n",
    "            random.shuffle(ind_shuff)\n",
    "            data=data[ind_shuff,:]\n",
    "            self.X=data[:,:-1]\n",
    "            Y=[int(t) for t in data[:,-1]]#.reshape(-1,1)\n",
    "            self.Y=one_hot(Y, nb_classes)\n",
    "            self.total_batch=int(np.floor(self.data_len/ self.batch_size))\n",
    "            \n",
    "        def Next_Batch(self):\n",
    "            batch_num = int(self.batch_num)\n",
    "            batch_size = int(self.batch_size)\n",
    "\n",
    "            data=self.X[batch_num * batch_size:(batch_num + 1) * batch_size,:]\n",
    "            label=self.Y[batch_num * batch_size:(batch_num + 1) * batch_size,:]\n",
    "            self.batch_num = self.batch_num + 1\n",
    "            if self.batch_num > self.total_batch:\n",
    "                self.batch_num = 0\n",
    "            return (data , label)\n",
    "\n",
    "            \n",
    "#get possible combinations from data_list\n",
    "data_list=[]\n",
    "for i in range(len(files)):\n",
    "    for j in np.arange(i+1,len(files)):\n",
    "        data_list.append([i,j])\n",
    "data_list=np.array(data_list)\n",
    "\n",
    "\n",
    "nb_classes = 2\n",
    "learning_rate = 0.01\n",
    "training_epochs = 45\n",
    "batch_size = 46\n",
    "data=DataRead(data_path,batch_size,nb_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "X=tf.placeholder(tf.float32,[None, 1440])\n",
    "X_img=tf.reshape(X,[-1,45,32,1])\n",
    "Y=tf.placeholder(tf.float32,[None,nb_classes])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([3,3,1,32],stddev=0.001))\n",
    "W2 = tf.Variable(tf.random_normal([3,3,32,64],stddev=0.001))\n",
    "W3 = tf.Variable(tf.random_normal([3,3,64,128],stddev=0.001))\n",
    "W4 = tf.get_variable(\"W4\", shape=[6 * 4 * 128, 625],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([625]))\n",
    "W5 = tf.get_variable(\"W5\", shape=[625, nb_classes],initializer=tf.contrib.layers.xavier_initializer())    \n",
    "b5 = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "with tf.name_scope(\"layer1\") as scope:    \n",
    "    L1=tf.nn.conv2d(X_img,W1,strides=[1,1,1,1],padding='SAME')\n",
    "    L1=tf.nn.relu(L1)\n",
    "    L1=tf.nn.max_pool(L1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    L1=tf.nn.dropout(L1,keep_prob=keep_prob)\n",
    "\n",
    "with tf.name_scope(\"layer2\") as scope:\n",
    "    L2=tf.nn.conv2d(L1,W2,strides=[1,1,1,1],padding='SAME')\n",
    "    L2=tf.nn.relu(L2)\n",
    "    L2=tf.nn.max_pool(L2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    L2=tf.nn.dropout(L2,keep_prob=keep_prob)\n",
    "\n",
    "with tf.name_scope(\"layer3\") as scope:\n",
    "    L3=tf.nn.conv2d(L2,W3,strides=[1,1,1,1],padding='SAME')\n",
    "    L3=tf.nn.relu(L3)\n",
    "    L3=tf.nn.max_pool(L3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "    L3=tf.reshape(L3,[-1,6*4*128])\n",
    "    L3=tf.nn.dropout(L3,keep_prob=keep_prob)\n",
    "\n",
    "with tf.name_scope(\"layer4\") as scope:\n",
    "    L4=tf.nn.relu(tf.matmul(L3,W4)+b4)\n",
    "    L4=tf.nn.dropout(L4,keep_prob=keep_prob)\n",
    "\n",
    "with tf.name_scope(\"layer5\") as scope:\n",
    "    logits=tf.matmul(L4,W5)+b5\n",
    "\n",
    "with tf.name_scope(\"cost\") as scope:\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "    cost_summ = tf.summary.scalar(\"cost\", cost)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "with tf.Session() as sess:  \n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer=tf.summary.FileWriter('/home/jeongjaepark/HistNet/logs/maps')\n",
    "    writer.add_graph(sess.graph)\n",
    "    step = 0\n",
    "    data.data_select([0,1,2,4,5,6,7,8,9])\n",
    "# train my model\n",
    "    print('Learning started. It takes sometime.')\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch =  data.train.total_batch\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = data.train.Next_Batch()\n",
    "            feed_dict = {X: batch_xs, Y: batch_ys,keep_prob:0.7}\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "            s=sess.run(merged_summary, feed_dict=feed_dict)\n",
    "            writer.add_summary(s, global_step=step)\n",
    "            step += 1\n",
    "        \n",
    "        \n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
