{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning started. It takes sometime.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from skimage import data, io, filters\n",
    "from mlxtend.preprocessing import one_hot\n",
    "import random\n",
    "\n",
    "# set train data path\n",
    "train_list_path = 'data/images_path/train.txt'\n",
    "train_data_path = 'data/images/train/%s'\n",
    "\n",
    "\n",
    "class DataRead:\n",
    "    def __init__(self, data_list_path, data_path, batch_size, nb_classes):\n",
    "        self.data_list_path = data_list_path\n",
    "        self.data_path = data_path\n",
    "        self.batch_num = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_classes = nb_classes\n",
    "\n",
    "        with open(self.data_list_path) as f:\n",
    "            self.data_list = f.readlines()\n",
    "        self.total_batch = int(np.floor(np.size(self.data_list) / self.batch_size))\n",
    "\n",
    "        self.label_list = [int(d.split()[1]) for d in self.data_list]\n",
    "        self.data_list = [train_data_path % d.split()[0] for d in self.data_list]\n",
    "\n",
    "        # shuffle data\n",
    "        ind_shuff = np.arange(np.size(self.data_list))\n",
    "        random.shuffle(ind_shuff)\n",
    "        self.data_list = [self.data_list[i] for i in ind_shuff]\n",
    "        self.label_list = [self.label_list[i] for i in ind_shuff]\n",
    "\n",
    "    def Next_Batch(self):\n",
    "        batch_num = int(self.batch_num)\n",
    "        batch_size = int(self.batch_size)\n",
    "\n",
    "        data_coll = io.ImageCollection(\n",
    "            self.data_list[batch_num * batch_size:(batch_num + 1) * batch_size])\n",
    "        data = np.array([d for d in data_coll])\n",
    "\n",
    "        label_coll = self.label_list[batch_num * batch_size:(batch_num + 1) * batch_size]\n",
    "        label = one_hot(label_coll, self.nb_classes)\n",
    "\n",
    "        self.batch_num = self.batch_num + 1\n",
    "        if self.batch_num > self.total_batch:\n",
    "            self.batch_num = 0\n",
    "        return (data.reshape([-1,256*256*3])/ 255.0, label)\n",
    "\n",
    "    def All_Data(self):\n",
    "        data_coll = io.ImageCollection(\n",
    "            self.data_list)\n",
    "        data = np.array([d for d in data_coll])\n",
    "\n",
    "        label_coll = self.label_list\n",
    "        label = one_hot(label_coll, self.nb_classes)\n",
    "\n",
    "        return (data.reshape([-1,256*256*3]), label)\n",
    "\n",
    "\n",
    "nb_classes = 13\n",
    "learning_rate = 0.001\n",
    "training_epochs = 45\n",
    "batch_size = 50\n",
    "train = DataRead(train_list_path, train_data_path, batch_size=batch_size, nb_classes=13)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 256*256*3])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[256*256*3, 128*128*3],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([128*128*3]))\n",
    "W2 = tf.get_variable(\"W2\", shape=[128*128*3, 32*32*3],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([32*32*3]))\n",
    "W3 = tf.get_variable(\"W3\", shape=[32*32*3, 32*32*3],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([32*32*3]))\n",
    "W4 = tf.get_variable(\"W4\", shape=[32*32*3, 16*16*3],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([16*16*3]))\n",
    "W5 = tf.get_variable(\"W5\", shape=[16*16*3, 8*8*3],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([8*8*3]))\n",
    "W6 = tf.get_variable(\"W6\", shape=[8*8*3, nb_classes],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "with tf.name_scope(\"layer1\") as scope:\n",
    "    # Hypothesis (using Relu)\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "    \n",
    "with tf.name_scope(\"layer2\") as scope:\n",
    "    # Hypothesis (using Relu)\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "with tf.name_scope(\"layer3\") as scope:\n",
    "    # Hypothesis (using Relu)\n",
    "    L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "    L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "    \n",
    "with tf.name_scope(\"layer4\") as scope:\n",
    "    # Hypothesis (using Relu)\n",
    "    L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "    L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "with tf.name_scope(\"layer5\") as scope:\n",
    "    # Hypothesis (using Relu)\n",
    "    L5 = tf.nn.relu(tf.matmul(L4, W5) + b5)\n",
    "    L5 = tf.nn.dropout(L5, keep_prob=keep_prob)\n",
    "\n",
    "with tf.name_scope(\"layer6\") as scope:\n",
    "    # Hypothesis (using softmax)\n",
    "    #hypothesis = tf.nn.softmax(tf.matmul(layer2, W3) + b3)\n",
    "    logits = tf.matmul(L5, W6) + b6\n",
    "    \n",
    "with tf.name_scope(\"cost\") as scope:\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "    cost_summ = tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.name_scope(\"prediction\") as scope:\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    accuracy_summ = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "# train my model\n",
    "sess=tf.Session()\n",
    "print('Learning started. It takes sometime.')\n",
    "merged_summary = tf.summary.merge_all()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "writer = tf.summary.FileWriter('./logs/KIST/DNN')\n",
    "writer.add_graph(sess.graph)\n",
    "step = 0\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    avg_a = 0\n",
    "    total_batch = train.total_batch\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = train.Next_Batch()\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _, a,s = sess.run([cost, optimizer, accuracy,merged_summary], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "        avg_a += a / total_batch\n",
    "\n",
    "        writer.add_summary(s, global_step=step)\n",
    "        step += 1\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost), 'accuracy =', '{:.9f}'.format(avg_a))\n",
    "\n",
    "val_list_path = 'data/images_path/val.txt'\n",
    "val_data_path = 'data/images/val/%s'\n",
    "val = DataRead(val_list_path, val_data_path, batch_size=100, nb_classes=13)\n",
    "\n",
    "val_x, val_y = val.Next_Batch()\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={X: val_x, Y: val_y, keep_prob: 1}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# train my model\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = train.total_batch    \n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = sess.run(train.Next_Batch())\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys,keep_prob:0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "val_list_path='data/images_path/val.txt'\n",
    "val_data_path='data/images/val/%s'\n",
    "val=DataRead(train_list_path,train_data_path,batch_size=batch_size,nb_classes=13)\n",
    "\n",
    "val_x, val_y=sess.run(val.Next_Batch())\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={X: val_x, Y: val_y,keep_prob:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.train.batch([train_label],batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "256*256*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:  \n",
    "    #train_label=sess.run(train_label)\n",
    "    train_data=sess.run(train_data[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b=sess.run(train.Next_Batch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[1:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from skimage import data, io,filters\n",
    "\n",
    "#set train data path\n",
    "train_list_path='data/images_path/train.txt'\n",
    "train_data_path='data/images/train/%s'\n",
    "\n",
    "#load train data path\n",
    "with open(train_list_path) as f:\n",
    "    train_data_list = f.readlines()    \n",
    "#load train data&label with path\n",
    "\n",
    "\n",
    "for i,tdl in enumerate(train_data_list):\n",
    "    if i==0:\n",
    "        train_data=io.imread(train_data_path%tdl.split()[0]).reshape([-1,256*256*3])\n",
    "        train_label=np.array([int(tdl.split()[1])])\n",
    "    elif i<100:\n",
    "        tmp=io.imread(train_data_path%tdl.split()[0]).reshape([-1,256*256*3])\n",
    "        train_data=tf.concat([train_data,tmp],0)\n",
    "        \n",
    "        tmp=np.array([int(tdl.split()[1])])\n",
    "        train_label=tf.concat([train_label,tmp],0)\n",
    "\n",
    "nb_classes = 13\n",
    "numb_of_neurons=98304\n",
    "learning_rate=0.001\n",
    "#convert train_label into one hot encoding\n",
    "train_label=tf.one_hot(train_label,nb_classes,axis=1)\n",
    "\n",
    "with tf.Session() as sess:  \n",
    "    train_label=sess.run(train_label)\n",
    "    train_data=sess.run(train_data)\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 256*256*3])# MNIST data image of shape 28 * 28 = 784\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])# 0 - 9 digits recognition = 10 classes\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[256*256*3, numb_of_neurons],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([numb_of_neurons]))\n",
    "W2 = tf.get_variable(\"W2\", shape=[numb_of_neurons, numb_of_neurons],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([numb_of_neurons]))\n",
    "W3 = tf.get_variable(\"W3\", shape=[numb_of_neurons, numb_of_neurons],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([numb_of_neurons]))\n",
    "W4 = tf.get_variable(\"W4\", shape=[numb_of_neurons, numb_of_neurons],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([numb_of_neurons]))\n",
    "W5 = tf.get_variable(\"W5\", shape=[numb_of_neurons, nb_classes],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "\n",
    "with tf.name_scope(\"layer1\") as scope:\n",
    "    # Hypothesis (using Relu)\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "    \n",
    "with tf.name_scope(\"layer2\") as scope:\n",
    "    # Hypothesis (using Relu)\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "with tf.name_scope(\"layer3\") as scope:\n",
    "    # Hypothesis (using Relu)\n",
    "    L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "    L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "    \n",
    "with tf.name_scope(\"layer4\") as scope:\n",
    "    # Hypothesis (using Relu)\n",
    "    L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "    L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "with tf.name_scope(\"layer5\") as scope:\n",
    "    # Hypothesis (using softmax)\n",
    "    #hypothesis = tf.nn.softmax(tf.matmul(layer2, W3) + b3)\n",
    "    hypothesis = tf.matmul(L4, W5) + b5\n",
    "    \n",
    "\n",
    "with tf.name_scope(\"cost\") as scope:\n",
    "    #cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "    cost_summ=tf.summary.scalar(\"cost\",cost)\n",
    "    \n",
    "with tf.name_scope(\"train\") as scope:    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(cost)\n",
    "    #GradientDescent doesn't work well.. I don't why Adam is better than Gradient.\n",
    "is_correct= tf.equal(tf.arg_max(hypothesis,1),tf.argmax(Y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "training_epochs=15\n",
    "batch_size=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from skimage import data, io, filters\n",
    "\n",
    "# set train data path\n",
    "train_list_path = 'data/images_path/train.txt'\n",
    "train_data_path = 'data/images/train/%s'\n",
    "\n",
    "\n",
    "class DataRead:\n",
    "    def __init__(self, data_list_path, data_path, batch_size, nb_classes):\n",
    "        self.data_list_path = data_list_path\n",
    "        self.data_path = data_path\n",
    "        self.batch_num = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_classes = nb_classes\n",
    "\n",
    "        with open(self.data_list_path) as f:\n",
    "            data_list = f.readlines()\n",
    "        self.total_batch = int(np.floor(np.size(data_list) / self.batch_size))\n",
    "\n",
    "        with open(self.data_list_path) as f:\n",
    "            self.data_list = f.readlines()\n",
    "\n",
    "\n",
    "    def Next_Batch(self):\n",
    "        first = 1\n",
    "        batch_num = self.batch_num\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        for i, tdl in enumerate(self.data_list):\n",
    "            if batch_num * batch_size <= i < (batch_num + 1) * batch_size:\n",
    "                if first:\n",
    "                    data = io.imread(self.data_path % tdl.split()[0]).reshape([-1, 256 * 256 * 3])\n",
    "                    label = np.array([int(tdl.split()[1])])\n",
    "                    first = 0\n",
    "                else:\n",
    "                    tmp = io.imread(self.data_path % tdl.split()[0]).reshape([-1, 256 * 256 * 3])\n",
    "                    data = tf.concat([data, tmp], 0)\n",
    "\n",
    "                    tmp = np.array([int(tdl.split()[1])])\n",
    "                    label = tf.concat([label, tmp], 0)\n",
    "        label = tf.one_hot(label, 13, axis=1)\n",
    "        self.batch_num = self.batch_num + 1\n",
    "        if self.batch_num > self.total_batch:\n",
    "            self.batch_num = 0\n",
    "        return (data, label)\n",
    "\n",
    "\n",
    "nb_classes = 13\n",
    "numb_of_neurons = 98304\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 30\n",
    "train = DataRead(train_list_path, train_data_path, batch_size=batch_size, nb_classes=13)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 256 * 256 * 3])\n",
    "X_img = tf.reshape(X, [-1, 256, 256, 3])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 3, 32], stddev=0.001))\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.001))\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.001))\n",
    "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L3 = tf.nn.relu(L3)\n",
    "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "L3 = tf.reshape(L3, [-1, 32 * 32 * 128])\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[32 * 32 * 128, 625], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([625]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[625, nb_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([nb_classes]))\n",
    "logits = tf.matmul(L4, W5) + b5\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = train.total_batch\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = sess.run(train.Next_Batch())\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "\n",
    "val_list_path='data/images_path/val.txt'\n",
    "val_data_path='data/images/val/%s'\n",
    "val=DataRead(train_list_path,train_data_path,batch_size=100,nb_classes=13)\n",
    "\n",
    "val_x, val_y=sess.run(val.Next_Batch())\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={X: val_x, Y: val_y,keep_prob:1}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from skimage import data, io, filters\n",
    "from mlxtend.preprocessing import one_hot\n",
    "import random\n",
    "\n",
    "# set train data path\n",
    "train_list_path = 'data/images_path/train.txt'\n",
    "train_data_path = 'data/images/train/%s'\n",
    "\n",
    "\n",
    "class DataRead:\n",
    "    def __init__(self, data_list_path, data_path, batch_size, nb_classes):\n",
    "        self.data_list_path = data_list_path\n",
    "        self.data_path = data_path\n",
    "        self.batch_num = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_classes = nb_classes\n",
    "\n",
    "        with open(self.data_list_path) as f:\n",
    "            self.data_list = f.readlines()\n",
    "        self.total_batch = int(np.floor(np.size(self.data_list) / self.batch_size))\n",
    "\n",
    "        self.label_list = [int(d.split()[1]) for d in self.data_list]\n",
    "        self.data_list = [train_data_path % d.split()[0] for d in self.data_list]\n",
    "        \n",
    "        #shuffle data\n",
    "        ind_shuff=np.arange(np.size(self.data_list))\n",
    "        random.shuffle(ind_shuff)\n",
    "        self.data_list=[self.data_list[i] for i in ind_shuff]\n",
    "        self.label_list=[self.label_list[i] for i in ind_shuff]\n",
    "\n",
    "    def Next_Batch(self):\n",
    "        batch_num = int(self.batch_num)\n",
    "        batch_size = int(self.batch_size)\n",
    "\n",
    "        data_coll = io.ImageCollection(\n",
    "            self.data_list[batch_num * batch_size:(batch_num + 1) * batch_size])\n",
    "        data = np.array([d for d in data_coll])\n",
    "\n",
    "        label_coll = self.label_list[batch_num * batch_size:(batch_num + 1) * batch_size]\n",
    "        label = one_hot(label_coll, self.nb_classes)\n",
    "\n",
    "        self.batch_num = self.batch_num + 1\n",
    "        if self.batch_num > self.total_batch:\n",
    "            self.batch_num = 0\n",
    "        return (data/255.0, label)\n",
    "\n",
    "    def All_Data(self):\n",
    "        data_coll = io.ImageCollection(\n",
    "            self.data_list)\n",
    "        data = np.array([d for d in data_coll])\n",
    "\n",
    "        label_coll = self.label_list\n",
    "        label = one_hot(label_coll, self.nb_classes)\n",
    "\n",
    "        return (data/255.0, label)\n",
    "\n",
    "\n",
    "nb_classes = 13\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 512\n",
    "train = DataRead(train_list_path, train_data_path, batch_size=batch_size, nb_classes=13)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 256,256,3])\n",
    "X_img = X\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "W1=tf.Variable(tf.random_normal([11,11,3,96],stddev=0.001))\n",
    "L1=tf.nn.conv2d(X_img,W1,strides=[1,4,4,1],padding='SAME')\n",
    "L1=tf.nn.relu(L1)\n",
    "L1=tf.nn.max_pool(L1,ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME')\n",
    "L1=tf.nn.dropout(L1,keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "W2=tf.Variable(tf.random_normal([5,5,96,256],stddev=0.001))\n",
    "L2=tf.nn.conv2d(L1,W2,strides=[1,1,1,1],padding='SAME')\n",
    "L2=tf.nn.relu(L2)\n",
    "L2=tf.nn.max_pool(L2,ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME')\n",
    "L2=tf.nn.dropout(L2,keep_prob=keep_prob)\n",
    "\n",
    "W3=tf.Variable(tf.random_normal([3,3,256,384],stddev=0.001))\n",
    "L3=tf.nn.conv2d(L2,W3,strides=[1,1,1,1],padding='SAME')\n",
    "L3=tf.nn.relu(L3)\n",
    "L3=tf.nn.dropout(L3,keep_prob=keep_prob)\n",
    "\n",
    "W4=tf.Variable(tf.random_normal([3,3,384,384],stddev=0.001))\n",
    "L4=tf.nn.conv2d(L3,W4,strides=[1,1,1,1],padding='SAME')\n",
    "L4=tf.nn.relu(L4)\n",
    "L4=tf.nn.dropout(L4,keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "W5=tf.Variable(tf.random_normal([3,3,384,256],stddev=0.001))\n",
    "L5=tf.nn.conv2d(L4,W5,strides=[1,1,1,1],padding='SAME')\n",
    "L5=tf.nn.relu(L5)\n",
    "L5=tf.nn.max_pool(L5,ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME')\n",
    "L5=tf.reshape(L5,[-1,8*8*256])\n",
    "L5=tf.nn.dropout(L5,keep_prob=keep_prob)\n",
    "\n",
    "W6 = tf.get_variable(\"W6\", shape=[8*8*256, 4096],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([4096]))\n",
    "L6 = tf.nn.relu(tf.matmul(L5,W6)+b6)\n",
    "L6 = tf.nn.dropout(L6,keep_prob=keep_prob)\n",
    "\n",
    "W7 = tf.get_variable(\"W7\", shape=[4096, 1000],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([1000]))\n",
    "L7 = tf.nn.relu(tf.matmul(L6,W7)+b7)\n",
    "L7 = tf.nn.dropout(L7,keep_prob=keep_prob)\n",
    "\n",
    "W8 = tf.get_variable(\"W8\", shape=[1000, 100],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([100]))\n",
    "L8 = tf.nn.relu(tf.matmul(L7,W8)+b8)\n",
    "L8 = tf.nn.dropout(L8,keep_prob=keep_prob)\n",
    "\n",
    "W9 = tf.get_variable(\"W9\", shape=[100, nb_classes],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b9 = tf.Variable(tf.random_normal([nb_classes]))\n",
    "logits=tf.matmul(L8,W9)+b9\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = train.total_batch\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = train.Next_Batch()\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "\n",
    "val_list_path='data/images_path/val.txt'\n",
    "val_data_path='data/images/val/%s'\n",
    "val=DataRead(val_list_path,val_data_path,batch_size=100,nb_classes=13)\n",
    "\n",
    "val_x, val_y=val.Next_Batch()\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={X: val_x, Y: val_y,keep_prob:1}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x, val_y=val.Next_Batch()\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={X: val_x, Y: val_y,keep_prob:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x, val_y=train.Next_Batch()\n",
    "a=sess.run(logits, feed_dict={X: val_x, Y: val_y,keep_prob:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4, 3, 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.batch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list_path = 'data/images_path/train.txt'\n",
    "train_data_path = 'data/images/train/%s'\n",
    "with open(train_list_path) as f:\n",
    "    data_list = f.readlines()\n",
    "label_list = [int(d.split()[1]) for d in data_list]\n",
    "data_list = [train_data_path % d.split()[0] for d in data_list]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "ind_shuff=np.arange(np.size(data_list),dtype='int32')\n",
    "random.shuffle(ind_shuff)\n",
    "label_list=[data_list[i] for i in ind_shuff]\n",
    "data_list=[label_list[i] for i in ind_shuff]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=[1,2 , 3,4,5,67,8,9,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=ind_shuff.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow1-1_pyth3",
   "language": "python",
   "name": "tensorflow1-1_pyth3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
